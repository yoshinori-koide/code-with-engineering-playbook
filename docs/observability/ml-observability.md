# 機械学習における可観測性

機械学習コンポーネントを備えたソフトウェアシステムの開発プロセスは、従来のソフトウェアよりも複雑です。コード、モデル、データの3つの次元で変化と変化を監視する必要があります。このようなシステムの寿命の2つの段階を区別できます。以下で説明するように、可観測性への異なるアプローチを必要とする実験と生産です。

## モデルの実験と調整

実験とは、1つ以上のデータセットを使用してそのようなモデルをトレーニングおよび評価することにより、適切な機械学習モデルとそのパラメーターを見つけるプロセスです。

機械学習モデルを開発および調整する場合、データサイエンティストは、さまざまなモデルパラメーターに対して選択されたパフォーマンスメトリックを観察および比較することに関心があります。また、特定のデータセットと特定のパラメーターが同じモデルを生成するように、トレーニングプロセスを再現するための信頼できる方法も必要です。

オープンソース（MLFlowなど）とプロプライエタリ（Azure Machine Learning Serviceなど）の両方で利用可能な多くのモデルメトリック評価ソリューションがあり、そのうちのいくつかは異なる目的を果たします。モデルメトリックをキャプチャするために、次のオプションを使用できます。

[Azure Machine Learning Service SDK](https://ml.azure.com/)
Azure Machine Learning Serviceは、評価指標をAzure Machine Learning Service（AML）実験に取り込むためのPython、R、およびC＃用のSDKを提供します。実験はAMLダッシュボードに表示されます。再現性は、コードまたはノートブックのスナップショットを表示されたメトリックと一緒に保存することで実現されます。AzureMachineLearningサービス内でバージョン管理されたデータセットを作成できます。

[MLFlow (for Databricks)](https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/)
MLFlowはオープンソースフレームワークであり、リモート追跡サーバーとしてAzure Databricksでホストできます（現在、Databricksとのファーストパーティ統合を提供する唯一のソリューションです）。MLFlow SDK追跡コンポーネントを使用して、評価メトリックまたは任意のパラメーターをキャプチャし、AzureDatabricksの実験ボードで追跡できます。ソースコードとデータセットのバージョンもログスナップショットとともに保存され、再現性を提供します。

[TensorBoard](https://www.tensorflow.org/tensorboard/)
TensorBoardは、データサイエンティストの間で人気のあるツールであり、ディープラーニングの実行、特にTensorFlowの実行の特定のメトリックを視覚化します。TensorBoardはAML/MLFlowのようなMLOpsツールではないため、広範なログ機能を提供しません。これは一時的なものです。したがって、AMLのようなエンドツーエンドのMLOpsツールへの追加として使用できますが、完全なMLOpsツールとしては使用できません。

[Application Insights](https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)
Application Insightsは、モデルメトリックをキャプチャするための代替シンクとして使用できるため、メトリックをPowerBIダッシュボードなどに転送できるため、より広範なオプションを提供できます。また、ログクエリを有効にします。ただし、このソリューションは、ログをAppInsightsに送信するためにカスタムアプリケーションを作成する必要があることを意味します（たとえば、OpenCensus Python SDKを使用）。これは、カスタムコードを作成/維持するための余分な労力を意味します。

4つのツールの広範な比較は、次のとおりです。

|                           | Azure ML      | MLFlow      | TensorBoard   | Application Insights |
| -----------               | ----------- | ----------- | -----------   | -----------          |
| **メトリクスのサポート**       | 値、画像、マトリックス、ログ | ファイルとしての値、画像、行列、プロット | DL調査フェーズに関連する指標 | 値、画像、マトリックス、ログ
| **カスタマイズ性**       | 基本 | 基本 | 非常に基本的 | 高い
| **メトリックへのアクセス性**    | AMLポータル、AML SDK | MLFlow UI、トラッキングサービスAPI | Tensorboard UI、履歴オブジェクト | Application Insights
| **ログへのアクセス性**       | BLOBまたはAMLポータルを介してアクセス可能なBLOBストレージ内の.txtファイルに書き込まれたローリングログ。クエリ不可 | ローリングログは保存されません | ローリングログは保存されませ | Azure PortalのApplication Insights。KQLでクエリ可能
| **使いやすさとセットアップ** | 非常に簡単で、ポータルは1つだけです | リモートトラッキングサーバーによる可動部品の増加 | プロセスのオーバーヘッドを少し超えています。MLフレームワークにも依存 | カスタムアプリとしてより多くの可動部品を維持する必要があります
| **共有可能性** | AMLワークスペースにアクセスできる人々全体 | リモート追跡サーバーにアクセスできる人々全体 | 同じディレクトリにアクセスできる人全体 | AppInsightsにアクセスできる人々全体

## 本番環境モデル

トレーニングされたモデルは、コンテナーとして本番環境にデプロイできます。Azure Machine Learningサービスは、モデルをAzure Container InstanceとしてデプロイするためのSDKを提供し、RESTエンドポイントを公開します。マイクロサービスの可観測性メソッドを使用して監視できます（詳細については、[レシピ](README.md)のセクションを参照してください）。MLFLowは、MLモデルをサービスとしてデプロイするための代替方法です。

## トレーニングと再トレーニング

モデルを自動的に再トレーニングするには、AMLパイプラインまたはAzureDatabricksを使用できます。AMLパイプラインを使用して再トレーニングする場合、Azureポータルの実験ダッシュボードの出力、ログ、さまざまなメトリックなど、各実行の情報を監視したり、AMLSDKを使用して手動で抽出したりできます。

## 時間の経過に伴うモデルのパフォーマンス：データドリフト

機械学習モデルを再トレーニングして、パフォーマンスを向上させ、時間の経過とともに変化するデータとモデルをより適切に整合させます。ただし、モデルのパフォーマンスが低下する場合があります。これは、データが劇的に変化し、モデル開発中に観察されたパターンが表示されなくなった場合に発生する可能性があります。この効果はデータドリフトと呼ばれます。Azure Machine Learning Serviceには、データのドリフトを監視および報告するためのプレビュー機能があります。この[記事](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets) ではそれについて詳しく説明します。

## データのバージョン管理

すべてのデータセットにバージョンを追加することをお勧めします。この目的のためにバージョン管理されたAzureMLデータセットを作成するか、他のシステムを使用している場合は手動でバージョン管理することができます。
